{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Search, Clustering and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Exploring nltk corpora\n",
    "\n",
    "The *nltk.corpus* module provides APIs for reading many corpora, including predefined ones downloaded using *ntlk.download()*. One of them is the gutenberg corpus containing about a dozen books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# fileids() tells us the file names in this corpus...\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "Use **raw()** to get the raw text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw() without arguments reads *all* the files in the corpus\n",
    "content = gutenberg.raw('austen-emma.txt')\n",
    "content[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "## Tokenization\n",
    "Use **words()** to read the file and get the list of tokens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']\n"
     ]
    }
   ],
   "source": [
    "words = gutenberg.words('austen-emma.txt')\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "Or **word_tokenize()** if raw text content is already available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2 = nltk.word_tokenize(content)\n",
    "words2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "Or its variant **wordpunct_tokenize()** which behaves like word_tokenize() but also treats all punctuation marks as separators..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'don', \"'\", 't']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words3 = nltk.wordpunct_tokenize('I don\\'t')\n",
    "words3[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Cleaning up the token stream\n",
    "\n",
    "Notice that **words()** has the following problems: \n",
    "\n",
    "- Marks like '[' and ']' are tokens, which while not incorrect, does not help us when it comes to document search or classification\n",
    "\n",
    "- Same words with different cases are treated as different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30815\n",
      "['[', ']', ',', ',', ',', ',', ',', ';', '-', '.']\n",
      "357\n",
      "4844\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any words without any alphanumeric characters.\n",
    "non_alphanumeric = [w for w in words if not w.isalnum()]\n",
    "print(len(non_alphanumeric))\n",
    "print(non_alphanumeric[:10])\n",
    "\n",
    "# Check if words with different cases are treated as different words.\n",
    "print(sum(1 for w in words if w=='The'))\n",
    "print(sum(1 for w in words if w=='the'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "We'll now clean up this token stream..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192427\n",
      "161612\n",
      "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma']\n"
     ]
    }
   ],
   "source": [
    "cleaned_up = [w.lower() for w in words if w.isalnum()]\n",
    "print(len(words))\n",
    "print(len(cleaned_up))\n",
    "print(cleaned_up[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "## Stop word removal\n",
    "\n",
    "*Stop words* are filler words like 'a', 'an', 'the', 'I', ... which in this use case are just noise for our tasks like search and classification, and can be removed.\n",
    "\n",
    "Every language has its own set of stop words, and nltk supports most European languages. We load the English stopwords set and list it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words from our tokens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73198\n",
      "['emma', 'jane', 'austen', '1816', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever']\n"
     ]
    }
   ],
   "source": [
    "filtered = [w for w in cleaned_up if w not in stop_words]\n",
    "print(len(filtered))\n",
    "print(filtered[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "But does it cover all stop words? Let's check by printing out the most frequent words in this filtered list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr', 1153), ('emma', 865), ('could', 837), ('would', 820), ('mrs', 699), ('miss', 599), ('must', 567), ('harriet', 506), ('much', 486), ('said', 484), ('one', 452), ('weston', 440), ('every', 435), ('well', 401), ('thing', 398), ('knightley', 389), ('elton', 385), ('think', 383), ('little', 359), ('good', 358), ('never', 358), ('know', 337), ('might', 326), ('woodhouse', 313), ('say', 310), ('jane', 301), ('quite', 282), ('time', 279), ('great', 264), ('nothing', 256)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(filtered).most_common(30),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "We seee that some filler words like 'would', 'could', 'said', 'might', 'much'... still occur frequently.\n",
    "\n",
    "Other words like 'mr', 'mrs', 'miss' .... are also borderline stopwords, but we'll retain them for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69945\n",
      "['emma', 'jane', 'austen', '1816', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever']\n",
      "[('mr', 1153), ('emma', 865), ('mrs', 699), ('miss', 599), ('harriet', 506), ('said', 484), ('one', 452), ('weston', 440), ('every', 435), ('well', 401), ('thing', 398), ('knightley', 389), ('elton', 385), ('think', 383), ('little', 359), ('good', 358), ('never', 358), ('know', 337), ('woodhouse', 313), ('say', 310), ('jane', 301), ('quite', 282), ('time', 279), ('great', 264), ('nothing', 256), ('dear', 241), ('fairfax', 241), ('always', 238), ('man', 235), ('thought', 226)]\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = ['could', 'would', 'must', 'will', 'much', 'might', 'shall', 'should']\n",
    "filtered = [w for w in filtered if w not in custom_stopwords]\n",
    "print(len(filtered))\n",
    "print(filtered[:10])\n",
    "from collections import Counter\n",
    "print(Counter(filtered).most_common(30),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "## Word counts / Term Frequencies\n",
    "\n",
    "We can either implement our own word counter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr', 1153), ('emma', 865), ('mrs', 699), ('miss', 599), ('harriet', 506), ('said', 484), ('one', 452), ('weston', 440), ('every', 435), ('well', 401), ('thing', 398), ('knightley', 389), ('elton', 385), ('think', 383), ('little', 359), ('good', 358), ('never', 358), ('know', 337), ('woodhouse', 313), ('say', 310), ('jane', 301), ('quite', 282), ('time', 279), ('great', 264), ('nothing', 256), ('dear', 241), ('fairfax', 241), ('always', 238), ('man', 235), ('thought', 226)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counter = Counter(filtered)\n",
    "print(word_counter.most_common(30),)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "...or use nltk's **FreqDist** class which is in fact a subclass of Counter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mr', 1153), ('emma', 865), ('mrs', 699), ('miss', 599), ('harriet', 506), ('said', 484), ('one', 452), ('weston', 440), ('every', 435), ('well', 401), ('thing', 398), ('knightley', 389), ('elton', 385), ('think', 383), ('little', 359), ('good', 358), ('never', 358), ('know', 337), ('woodhouse', 313), ('say', 310), ('jane', 301), ('quite', 282), ('time', 279), ('great', 264), ('nothing', 256), ('dear', 241), ('fairfax', 241), ('always', 238), ('man', 235), ('thought', 226)]\n"
     ]
    }
   ],
   "source": [
    "freqs = nltk.FreqDist(filtered)\n",
    "print(freqs.most_common(30),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "## Combine everything done so far into a reusable preprocessing function\n",
    "\n",
    "For document search or classification, we need to execute all these steps like deduplication and stop word removal  on every document in a corpus. Combine it all into a function that takes a document as input and returns terms and their frequencies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emma', 855), ('miss', 597), ('harriet', 496), ('said', 483), ('one', 447), ('every', 434), ('weston', 430), ('thing', 394), ('think', 383), ('elton', 378), ('well', 375), ('knightley', 373), ('little', 359), ('never', 358), ('know', 335), ('good', 313), ('say', 310), ('woodhouse', 308), ('jane', 299), ('quite', 282), ('time', 275), ('great', 263), ('nothing', 252), ('dear', 241), ('always', 238), ('man', 232), ('fairfax', 232), ('thought', 225), ('soon', 223), ('see', 222)]\n"
     ]
    }
   ],
   "source": [
    "def term_frequencies(doc):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    cleaned_up = [w.lower() for w in words if w.isalnum()]\n",
    "    filtered = cleaned_up\n",
    "    filtered = [w for w in cleaned_up if w not in stop_words]\n",
    "    filtered = [w for w in filtered if w not in custom_stopwords]\n",
    "    freqs = nltk.FreqDist(filtered)\n",
    "    return freqs\n",
    "\n",
    "# test it...\n",
    "freqs = term_frequencies(content)\n",
    "print(freqs.most_common(30),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "-----\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Section 2 - Understanding TF-IDF\n",
    "\n",
    "In this section, we understand the concept of TF-IDF, why it's necessary and how to calculate it.\n",
    "\n",
    "But before that, let's understand the problems themselves - namely document searching, clustering and classification.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Understanding the Document Search problem\n",
    "\n",
    "We have a set of documents. Given a search query, we have to find documents that \"match\" the query. Additionally, the user expects the query results to be relevant as well as sorted with the most relevant search results at the top.\n",
    "How can this problem be solved?\n",
    "\n",
    "Let's take an example...\n",
    "\n",
    "- Search query: *\"book on quantum physics\"*\n",
    "\n",
    "- Document #1: *\"A is a good book on physics. B is another good book on physics. C is a good book on algorithms.\"*\n",
    "\n",
    "- Document #2: *\"D is a well written book and covers quantum physics in detail.\"*\n",
    "\n",
    "- Document #3: *\"E is a great course on quantum physics.\"*\n",
    "\n",
    "\n",
    "Clearly, #2 is the most relevant. But if we use just the word counts and score each document based on number of occurrences of search terms in it, then #1 will score higher simply because the word 'book' occurs more often.\n",
    "\n",
    "We need a different scoring that penalizes too many occurrences of common words while rewarding few occurrences of rare words. This is where TF-IDF helps...\n",
    "\n",
    "\n",
    "\n",
    "### What is TF-IDF? And how does it help?\n",
    "\n",
    "**TF** stands for **Term Frequences** which we have already calculated previously.\n",
    "\n",
    "**IDF** is **Inverse Document Frequency**. Every *term* has an IDF score. Conceptually, IDF is a measure of the ratio of (total number of documents in the corpus) to (number of documents that contain that term). \n",
    "Mathematically, it's defined as \n",
    "$$IDF_{term} = log \\frac{numdocs_{total}}{1 + numdocs_{term}}$$\n",
    "\n",
    "We can see that if $numdocs_{term} \\approx numdocs_{total}$, IDF becomes log 1 = 0. So a term that is so common that it occurs in (nearly) every document in the corpus gets a very low, near-zero IDF weight.\n",
    "\n",
    "Conversely, a rare term that occurs in only a few documents, such that $numdocs_{term} \\lt\\lt numdocs_{total}$, makes its IDF score close to its maximum.\n",
    "\n",
    "Note that if a term does not occur at all in any document, then the IDF is maximum which is silly. The 1 in denominator is to avoid division by zero in case a term does not occur in any document. Such a term should simply be ignored, since its term frequency and IDF will be zero in all documents.\n",
    "\n",
    "The **TF-IDF score** of a term in a document is the product of its (TF in that document) and (IDF across documnents).\n",
    "\n",
    "Instead of describing a document as a vector of word counts, we describe it as a vector of tf-idf scores containing a score for every term across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6809\n",
      "5561\n",
      "6137\n",
      "12434\n",
      "1378\n",
      "3622\n",
      "1346\n",
      "2340\n",
      "7914\n",
      "7387\n",
      "6052\n",
      "8055\n",
      "16561\n",
      "8751\n",
      "2836\n",
      "4452\n",
      "3204\n",
      "11879\n"
     ]
    }
   ],
   "source": [
    "all_terms = {}\n",
    "doc_freqs = {}\n",
    "for f in gutenberg.fileids():\n",
    "    doc = gutenberg.raw(f)\n",
    "    freqs = term_frequencies(doc)\n",
    "    doc_freqs[f] = freqs\n",
    "    print(len(freqs))\n",
    "    for term in freqs.keys():\n",
    "        if term in all_terms:\n",
    "            all_terms[term] += 1\n",
    "        else:\n",
    "            all_terms[term] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40897"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tier 2 0.7781512503836436\n",
      "stephanas 1 0.9542425094393249\n",
      "ahijah 1 0.9542425094393249\n",
      "recrossing 1 0.9542425094393249\n",
      "searches 2 0.7781512503836436\n",
      "senegal 1 0.9542425094393249\n",
      "suppliant 1 0.9542425094393249\n",
      "dallying 2 0.7781512503836436\n",
      "cowardice 3 0.6532125137753437\n",
      "methegammah 1 0.9542425094393249\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "idf_scores = {}\n",
    "for t in list(all_terms):\n",
    "    num_docs_with_term = all_terms[t]\n",
    "    idf_scores[t] = math.log10(N / (1 + num_docs_with_term))\n",
    "    \n",
    "for t in list(idf_scores)[:10]:\n",
    "    print(t, all_terms[t], idf_scores[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('know', -0.023481095849522914), ('heard', -0.023481095849522914), ('saying', -0.023481095849522914), ('day', -0.023481095849522914), ('found', -0.023481095849522914)]\n",
      "[('biblic', 0.9542425094393249), ('intercedings', 0.9542425094393249), ('wineglasses', 0.9542425094393249), ('demetrius', 0.9542425094393249), ('eleve', 0.9542425094393249)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "sorted_scores = sorted(idf_scores.items(), key=operator.itemgetter(1))\n",
    "print(sorted_scores[:5])\n",
    "print(sorted_scores[-6:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "decree (0.4101744650890493, 1, 0.4101744650890493)\n",
      "stop (0.0, 15, 0.0)\n",
      "reproach (3.1696426630022625, 9, 0.3521825181113625)\n",
      "look (-2.794250406093227, 119, -0.023481095849522914)\n",
      "military (0.25527250510330607, 1, 0.25527250510330607)\n",
      "damps (0.7781512503836436, 1, 0.7781512503836436)\n",
      "strictly (0.5105450102066121, 2, 0.25527250510330607)\n",
      "walk (0.0, 53, 0.0)\n",
      "parlours (1.5563025007672873, 2, 0.7781512503836436)\n",
      "consternation (0.8203489301780986, 2, 0.4101744650890493)\n",
      "\n",
      "whitman-leaves.txt\n",
      "tier (1.5563025007672873, 2, 0.7781512503836436)\n",
      "traitors (1.0565475543340874, 3, 0.3521825181113625)\n",
      "senegal (0.9542425094393249, 1, 0.9542425094393249)\n",
      "dallying (3.1126050015345745, 4, 0.7781512503836436)\n",
      "strictly (0.25527250510330607, 1, 0.25527250510330607)\n",
      "walk (0.0, 66, 0.0)\n",
      "florida (5.447058752685505, 7, 0.7781512503836436)\n",
      "laughingly (0.9542425094393249, 1, 0.9542425094393249)\n",
      "coasts (2.225210003069149, 4, 0.5563025007672873)\n",
      "airs (2.4082399653118496, 8, 0.3010299956639812)\n",
      "\n",
      "melville-moby_dick.txt\n",
      "czar (4.668907502301861, 6, 0.7781512503836436)\n",
      "searches (0.7781512503836436, 1, 0.7781512503836436)\n",
      "uncounted (2.8627275283179747, 3, 0.9542425094393249)\n",
      "saratoga (1.9084850188786497, 2, 0.9542425094393249)\n",
      "decree (0.4101744650890493, 1, 0.4101744650890493)\n",
      "military (1.5316350306198365, 6, 0.25527250510330607)\n",
      "soliloquizer (0.9542425094393249, 1, 0.9542425094393249)\n",
      "strictly (1.7869075357231425, 7, 0.25527250510330607)\n",
      "walk (0.0, 8, 0.0)\n",
      "consternation (4.101744650890494, 10, 0.4101744650890493)\n",
      "\n",
      "bryant-stories.txt\n",
      "roadside (1.1126050015345745, 2, 0.5563025007672873)\n",
      "stop (0.0, 7, 0.0)\n",
      "quite (0.0, 27, 0.0)\n",
      "look (-1.0801304090780541, 46, -0.023481095849522914)\n",
      "reproduction (0.9542425094393249, 1, 0.9542425094393249)\n",
      "walk (0.0, 7, 0.0)\n",
      "resolved (0.21387981994508107, 1, 0.21387981994508107)\n",
      "hedge (0.17609125905568124, 1, 0.17609125905568124)\n",
      "scratched (0.47712125471966244, 1, 0.47712125471966244)\n",
      "cuckoo (1.9084850188786497, 2, 0.9542425094393249)\n",
      "\n",
      "bible-kjv.txt\n",
      "stephanas (2.8627275283179747, 3, 0.9542425094393249)\n",
      "ahijah (19.084850188786497, 20, 0.9542425094393249)\n",
      "mijamin (1.9084850188786497, 2, 0.9542425094393249)\n",
      "heres (0.7781512503836436, 1, 0.7781512503836436)\n",
      "smiths (2.612850055101375, 4, 0.6532125137753437)\n",
      "matred (1.9084850188786497, 2, 0.9542425094393249)\n",
      "walk (0.0, 212, 0.0)\n",
      "parlours (0.7781512503836436, 1, 0.7781512503836436)\n",
      "arvad (1.9084850188786497, 2, 0.9542425094393249)\n",
      "plowers (0.9542425094393249, 1, 0.9542425094393249)\n",
      "\n",
      "austen-persuasion.txt\n",
      "torment (0.6020599913279624, 2, 0.3010299956639812)\n",
      "stop (0.0, 4, 0.0)\n",
      "look (-1.3149413675732833, 56, -0.023481095849522914)\n",
      "bending (0.21387981994508107, 1, 0.21387981994508107)\n",
      "walk (0.0, 35, 0.0)\n",
      "resolved (0.21387981994508107, 1, 0.21387981994508107)\n",
      "hedge (0.5282737771670437, 3, 0.17609125905568124)\n",
      "airs (0.3010299956639812, 1, 0.3010299956639812)\n",
      "amicable (0.6532125137753437, 1, 0.6532125137753437)\n",
      "london (1.97860813915057, 14, 0.1413291527964693)\n",
      "\n",
      "chesterton-ball.txt\n",
      "cowardice (1.3064250275506875, 2, 0.6532125137753437)\n",
      "stop (0.0, 21, 0.0)\n",
      "look (-0.821838354733302, 35, -0.023481095849522914)\n",
      "military (1.2763625255165303, 5, 0.25527250510330607)\n",
      "mirthless (0.9542425094393249, 1, 0.9542425094393249)\n",
      "strictly (0.7658175153099183, 3, 0.25527250510330607)\n",
      "walk (0.0, 13, 0.0)\n",
      "florida (0.7781512503836436, 1, 0.7781512503836436)\n",
      "resolved (0.42775963989016214, 2, 0.21387981994508107)\n",
      "hedge (1.0565475543340874, 6, 0.17609125905568124)\n",
      "\n",
      "shakespeare-macbeth.txt\n",
      "traitors (1.0565475543340874, 3, 0.3521825181113625)\n",
      "stop (0.0, 2, 0.0)\n",
      "quite (0.0, 1, 0.0)\n",
      "look (-0.07044328754856874, 3, -0.023481095849522914)\n",
      "walk (0.0, 2, 0.0)\n",
      "tuesday (0.4101744650890493, 1, 0.4101744650890493)\n",
      "foundations (0.3521825181113625, 1, 0.3521825181113625)\n",
      "sleeping (0.3521825181113625, 2, 0.17609125905568124)\n",
      "prediction (0.6532125137753437, 1, 0.6532125137753437)\n",
      "monkie (0.9542425094393249, 1, 0.9542425094393249)\n",
      "\n",
      "austen-sense.txt\n",
      "torment (0.3010299956639812, 1, 0.3010299956639812)\n",
      "decree (0.4101744650890493, 1, 0.4101744650890493)\n",
      "stop (0.0, 8, 0.0)\n",
      "look (-1.5967145177675581, 68, -0.023481095849522914)\n",
      "strictly (0.7658175153099183, 3, 0.25527250510330607)\n",
      "walk (0.0, 20, 0.0)\n",
      "resolved (5.98863495846227, 28, 0.21387981994508107)\n",
      "improperly (0.7781512503836436, 1, 0.7781512503836436)\n",
      "foundations (0.3521825181113625, 1, 0.3521825181113625)\n",
      "unaccountably (0.5563025007672873, 1, 0.5563025007672873)\n",
      "\n",
      "burgess-busterbrown.txt\n",
      "scrambling (0.5563025007672873, 1, 0.5563025007672873)\n",
      "death (-0.023481095849522914, 1, -0.023481095849522914)\n",
      "stop (0.0, 3, 0.0)\n",
      "quite (0.0, 8, 0.0)\n",
      "scamp (1.5563025007672873, 2, 0.7781512503836436)\n",
      "far (0.24823583725032144, 10, 0.024823583725032145)\n",
      "spoil (0.3521825181113625, 2, 0.17609125905568124)\n",
      "wings (0.3274334082752043, 3, 0.10914446942506809)\n",
      "walk (0.0, 2, 0.0)\n",
      "grave (0.07918124604762482, 1, 0.07918124604762482)\n",
      "\n",
      "shakespeare-caesar.txt\n",
      "decree (0.8203489301780986, 2, 0.4101744650890493)\n",
      "cowardice (0.6532125137753437, 1, 0.6532125137753437)\n",
      "traitors (3.1696426630022625, 9, 0.3521825181113625)\n",
      "stop (0.0, 1, 0.0)\n",
      "quite (0.0, 4, 0.0)\n",
      "look (-0.11740547924761457, 5, -0.023481095849522914)\n",
      "bending (0.21387981994508107, 1, 0.21387981994508107)\n",
      "walk (0.0, 3, 0.0)\n",
      "pawse (0.7781512503836436, 1, 0.7781512503836436)\n",
      "hauocke (0.7781512503836436, 1, 0.7781512503836436)\n",
      "\n",
      "milton-paradise.txt\n",
      "fixes (0.7781512503836436, 1, 0.7781512503836436)\n",
      "decree (6.1526169763357395, 15, 0.4101744650890493)\n",
      "involved (1.5316350306198365, 6, 0.25527250510330607)\n",
      "stop (0.0, 5, 0.0)\n",
      "look (-0.5165841086895041, 22, -0.023481095849522914)\n",
      "bending (0.42775963989016214, 2, 0.21387981994508107)\n",
      "suppliant (1.9084850188786497, 2, 0.9542425094393249)\n",
      "strictly (0.7658175153099183, 3, 0.25527250510330607)\n",
      "walk (0.0, 14, 0.0)\n",
      "compeer (0.9542425094393249, 1, 0.9542425094393249)\n",
      "\n",
      "blake-poems.txt\n",
      "clothed (0.6020599913279624, 2, 0.3010299956639812)\n",
      "look (-0.11740547924761457, 5, -0.023481095849522914)\n",
      "contagious (0.5563025007672873, 1, 0.5563025007672873)\n",
      "walk (0.0, 3, 0.0)\n",
      "bags (1.1126050015345745, 2, 0.5563025007672873)\n",
      "terrors (0.3521825181113625, 1, 0.3521825181113625)\n",
      "sleeping (1.0565475543340874, 6, 0.17609125905568124)\n",
      "london (0.2826583055929386, 2, 0.1413291527964693)\n",
      "girl (0.43657787770027234, 4, 0.10914446942506809)\n",
      "silence (0.05115252244738129, 1, 0.05115252244738129)\n",
      "\n",
      "shakespeare-hamlet.txt\n",
      "heres (0.7781512503836436, 1, 0.7781512503836436)\n",
      "respeaking (0.9542425094393249, 1, 0.9542425094393249)\n",
      "stop (0.0, 3, 0.0)\n",
      "quite (0.0, 4, 0.0)\n",
      "look (-0.1643676709466604, 7, -0.023481095849522914)\n",
      "milkie (0.9542425094393249, 1, 0.9542425094393249)\n",
      "weedy (0.6532125137753437, 1, 0.6532125137753437)\n",
      "marcell (0.9542425094393249, 1, 0.9542425094393249)\n",
      "courtiers (1.1126050015345745, 2, 0.5563025007672873)\n",
      "passeth (0.7781512503836436, 1, 0.7781512503836436)\n",
      "\n",
      "carroll-alice.txt\n",
      "chains (0.3010299956639812, 1, 0.3010299956639812)\n",
      "stop (0.0, 6, 0.0)\n",
      "quite (0.0, 55, 0.0)\n",
      "buttered (0.6532125137753437, 1, 0.6532125137753437)\n",
      "look (-0.6339895879371187, 27, -0.023481095849522914)\n",
      "walk (0.0, 5, 0.0)\n",
      "alice (258.6721554550361, 396, 0.6532125137753437)\n",
      "fender (0.9542425094393249, 1, 0.9542425094393249)\n",
      "afraid (0.6138302693685755, 12, 0.05115252244738129)\n",
      "airs (0.3010299956639812, 1, 0.3010299956639812)\n",
      "\n",
      "chesterton-brown.txt\n",
      "tier (1.5563025007672873, 2, 0.7781512503836436)\n",
      "stop (0.0, 9, 0.0)\n",
      "look (-1.5967145177675581, 68, -0.023481095849522914)\n",
      "military (1.7869075357231425, 7, 0.25527250510330607)\n",
      "carr (0.9542425094393249, 1, 0.9542425094393249)\n",
      "strictly (1.2763625255165303, 5, 0.25527250510330607)\n",
      "walk (0.0, 8, 0.0)\n",
      "tourniquet (0.9542425094393249, 1, 0.9542425094393249)\n",
      "cutlass (7.003361253452793, 9, 0.7781512503836436)\n",
      "consternation (0.4101744650890493, 1, 0.4101744650890493)\n",
      "\n",
      "edgeworth-parents.txt\n",
      "torment (0.6020599913279624, 2, 0.3010299956639812)\n",
      "searches (0.7781512503836436, 1, 0.7781512503836436)\n",
      "cowardice (0.6532125137753437, 1, 0.6532125137753437)\n",
      "traitors (0.704365036222725, 2, 0.3521825181113625)\n",
      "stop (0.0, 37, 0.0)\n",
      "look (-4.273559444613171, 182, -0.023481095849522914)\n",
      "bending (0.21387981994508107, 1, 0.21387981994508107)\n",
      "strictly (1.2763625255165303, 5, 0.25527250510330607)\n",
      "walk (0.0, 35, 0.0)\n",
      "folio (0.7781512503836436, 1, 0.7781512503836436)\n",
      "\n",
      "chesterton-thursday.txt\n",
      "czar (3.1126050015345745, 4, 0.7781512503836436)\n",
      "civilised (0.9542425094393249, 1, 0.9542425094393249)\n",
      "pinnacle (0.6532125137753437, 1, 0.6532125137753437)\n",
      "traitors (0.3521825181113625, 1, 0.3521825181113625)\n",
      "stop (0.0, 13, 0.0)\n",
      "thinking (0.14894150235019288, 6, 0.024823583725032145)\n",
      "look (-1.103611504927577, 47, -0.023481095849522914)\n",
      "military (0.7658175153099183, 3, 0.25527250510330607)\n",
      "strictly (0.25527250510330607, 1, 0.25527250510330607)\n",
      "walk (0.0, 13, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectors = {}\n",
    "least_tf_idf = (None, 0.0, 0.0, 0.0, None)\n",
    "highest_tf_idf = (None, 0.0, 0.0, 0.0)\n",
    "for f in list(doc_freqs):\n",
    "    term_freqs = doc_freqs[f]\n",
    "    tf_idf_vectors[f] = {}\n",
    "    for t in list(term_freqs):\n",
    "        tf_idf_score_for_term = term_freqs[t] * idf_scores[t]\n",
    "        tf_idf_vectors[f][t] = (tf_idf_score_for_term, term_freqs[t], idf_scores[t])\n",
    "        \n",
    "        if tf_idf_score_for_term < least_tf_idf[1]:\n",
    "            least_tf_idf = (t, tf_idf_score_for_term, term_freqs[t], idf_scores[t], f)\n",
    "            \n",
    "        if tf_idf_score_for_term > highest_tf_idf[1]:\n",
    "            highest_tf_idf = (t, tf_idf_score_for_term, term_freqs[t], idf_scores[t], f)\n",
    "        \n",
    "        \n",
    "for f,tf_idf_vector in tf_idf_vectors.items():\n",
    "    print(f)\n",
    "    for t in list(tf_idf_vector)[:10]:\n",
    "        print(t, tf_idf_vector[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('said', -93.90090230224213, 3999, -0.023481095849522914, 'bible-kjv.txt')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('unto', 3690.3396624061766, 8997, 0.4101744650890493, 'bible-kjv.txt')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
